---
title: "Homeworks"
author: "Zhang Jin 20007"
date: "2020/12/18"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Homeworks}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Homework 1 (2020-09-22)

## Question

Use knitr to produce 3 examples in the book. The 1st example should contain texts and at least one figure. The 2nd example should contains texts and at least one table. The 3rd example should contain at least a couple of LaTeX formulas.

## Answer

### (1)
In my first example, I used lattice package to make density plot. For the randomly generated data, I plotted both empirical density curve and normal fitting curve.

```{r}
library(lattice)
n <- seq(5, 45, 5)
x <- rnorm(sum(n))
y <- factor(rep(n, n), labels=paste("n =", n))
densityplot(~ x | y,
panel = function(x, ...) {
panel.densityplot(x, col="DarkOliveGreen", ...)
panel.mathdensity(dmath=dnorm,
args=list(mean=mean(x), sd=sd(x)),
col="darkblue")
})

```

### (2)
In my second example, I used some methods to deal with the longitudinal data of some patients.


```{r}
stdv = function(x) {  if (NROW(x)>1)   sigma <- round(sd(x),2) 
                                   else   sigma <- 0
                                   sigma }
smry <- function(X){     mu <- apply(X,2,mean)  
sigma <- apply(X,2,stdv)
c(mu=mu,sigma=sigma)}

patients <- c("0071021198307012008001400400150",
                       "0071201198307213009002000500200",
                      "0090903198306611007013700300000",
                      "0050705198307414008201300900000",
                     "0050115198208018009601402001500",
                     "0050618198207017008401400800400",
                     "0050703198306414008401400800200")
id <- substr(patients,1,3)    # First 3 digits signify the patient ID
date <- as.Date(substr(patients,4,11), format = "%m%d%Y")    # 4-11 for treatment date
hr <- as.numeric(substr(patients,12,14))    # Heart rate
sbp <- as.numeric(substr(patients,15,17))    # Systolic blood pressure
dbp <- as.numeric(substr(patients,18,20))    # Diastolic blood pressure
dx <- substr(patients,21,23)
docfee <- as.numeric(substr(patients,24,27))  
labfee <- as.numeric(substr(patients,28,31))

tapply(hr, id, mean)
tapply(hr, id, stdv)

# Show these results in a more compact way
PATIENTS <- data.frame(id, hr, sbp, dbp, docfee, labfee)
str(PATIENTS)
smry(PATIENTS[id=='005',2:6])
smry(PATIENTS[id=='007',2:6])
smry(PATIENTS[id=='009',2:6])
by(PATIENTS[2:6], id, smry) 
by(PATIENTS[2:6], id, summary)

# Calculate the difference between the first and the last observations of HR, SBP and DBP
HrSbpDbp  <- data.frame(id, date, hr, sbp, dbp)
# Sort by ID first, then sort by treatment date
HrSbpDbpSorted <- HrSbpDbp[order(HrSbpDbp$id, HrSbpDbp$date), ] 
HrSbpDbpSorted
```

### (3)
In this example, I listed several theorems using some Latex copulas.  

Levy CLT:  

$\displaystyle\frac{\sqrt{n}(\bar{X} - \mu)}{\sigma} \stackrel{d}{\longrightarrow} N(0,1)$  
  
Berry-Esseen CLT:  
  
$\displaystyle\sup_{t} |G_n(t)-\Phi(t)| \leq C \frac{\rho}{\sigma^3 \sqrt{n}}$  
  
Percentile CLT:  

$\displaystyle\lim_{n \rightarrow \infty} P \Big( \frac{\sqrt{n} (\hat{\xi}_{pn}-\xi_p)}{\sqrt{p(1-p)}/F'(\xi_p -)} \leq t \Big) = \Phi(t)$  

$~$  

$~$

# Homework 2 (2020-09-29)

## Exercise 3.3
### Problem
The Pareto(a, b) distribution has cdf   

$$\displaystyle F(x)=1-(\frac{b}{x})^a,~~~~~~~x \geq b >0,~a>0.$$  

Derive the probability inverse transformation $F^{-1}(U)$ and use the inverse transform method to simulate a random sample from the Pareto(2,2) distribution. Graph the density histogram of the sample with the Pareto(2,2) density superimposed for comparison.




### Solution

First, for the cdf  

$$\displaystyle F(x)=1-(\frac{b}{x})^a,~~~~~~~x \geq b >0,~a>0$$,  

I can derive its inverse  

$$\displaystyle F^{-1}(x)=\frac{b}{\sqrt[a]{1-x}}~~~~~~~0 \leq x \leq 1,~a>0$$,  

Then I could generate random variables from Uniform(0,1).  

I will simulate a sample number of 10000, $a=b=2$ as the exercise suggests.  
The pdf of the Pareto(a,b) distribution is  

$$f(x)=\displaystyle \frac{ab^a}{x^{a+1}},~~~~~~~x \geq b >0,~a>0$$ 


```{r}

n <- 10000
a <- 2
b <- 2
unifrv <- runif(n)
paretorv <- b/(1-unifrv)^(1/a)    # inverse transformation
hist(paretorv[paretorv>0 & paretorv<20], freq = FALSE, breaks = seq(0,20,0.5), main = "Histogram of the Pareto sample with the true density curve",xlab = "Sample value")    # graph the density histogram
# I found that F(20)=0.99, which is very close to 1. For the sake of the tidiness and better description for the feature of the histogram, I made a truncation on x=20, and only consider the variables between 0 and 20.
f <- function(x) {a*b^a/x^(a+1)}    # true pdf
curve(f, 2, 20, col = 2, lwd = 3, add = TRUE)    # add the true density curve
legend(12,0.6,"true density", col = 2, lwd = 3)    # add a legend

```

## Exercise 3.9
### Problem
The rescaled Epanechnikov kernel is a symmetric density function  

$$\displaystyle f_e(x)=\frac{3}{4}(1-x^2),~~~~~~~|x| \leq 1$$  

Devroye and Gyorfi give the following algorithm for simulation from this distribution. Generate iid $U_1,U_2,U_3 \sim$ Uniform(-1,1). If $|U_3|\geq|U_2|$ and $|U_3|\geq|U_1|$, deliver $U_2$; otherwise deliver |U_3|. Write a function to generate random variates from $f_e$, and construct the histogram density estimate of a large simulated random sample.

### Solution

Just do as the problem suggests. Consider a sample number of 10000.
```{r}
u1 <- runif(10000, min = -1, max = 1)    # consider a sample number of 10000
u2 <- runif(10000, min = -1, max = 1)
u3 <- runif(10000, min = -1, max = 1)
u <- ifelse((abs(u3)>abs(u2) & abs(u3)>abs(u1)), u2, u3)
hist(u, freq = FALSE, breaks = seq(-1,1,0.02), main = "Histogram with the true density curve", xlab = "Sample value")
f <- function(x) {3/4*(1-x^2)}
curve(f, -1, 1, col = 2, lwd = 3, add = TRUE)    # add the true density curve
legend(0.5,0.85,"true density", col = 2, lwd = 3, cex=0.6)    # add a legend

```

## Exercise 3.10
### Problem
Prove that the algorithm given in Exercise 3.9 generates variates from the density $f_e$

### Solution
Let $X_1=|U_1|, X_2=|U_2|, X_3=|U_3|$,then clearly there is $X_1,X_2,X_3 \sim$ Uniform(0,1).  

Let $X= \begin{cases}
X_2 & ~~X_3 \geq X_1~~and~~ X_3 \geq X_2 \\
X_3 & ~~otherwise \\
\end{cases}$  
Then, since $f_e(x)$ is symmetric, all I have to prove is that $X$ obeys the pdf  
$$f_{e'}(x)=2*\frac{3}{4}(1-x^2),~~~~~~~0 \leq x \leq 1$$  

Now, notice that the algorithm to generate $X$ can be rewritten as:  

1. Generate $X_1,X_2,X_3 \sim$ Uniform(0,1).  
2. Remove the largest value $X_{(3)}$  
3. Select one of the remaining two values with equal probability. Let $X$ be this value.  

For any $0 \leq x \leq 1$, consider the events:  
$A=\{$ only one of the $X_i \leq x$ $\}$  
$B=\{$ at least two of the $X_i \leq x$ $\}$,  
Then, it's easy to compute the cdf of $X$, $F_{e'}(x)$ as following:

\begin{align}
F_{e'}(x)&=P(X\leq x)\\
&=P(X \leq x,A)+P(X \leq x,B)\\
&=P(X \leq x~|~A)*P(A) + P(X \leq x~|~B)*P(B)\\
&=\frac{1}{2}*3x(1-x)^2 + 1*[3x^2(1-x) + x^3]\\
&=-\frac{1}{2} x^3 +\frac{3}{2}x,~~~~~~0 \leq x \leq 1
\end{align}  

Thus, the pdf of $X$ is 
$$ f_{e'}(x)=F'_{e'}(x)=2*\frac{3}{4}(1-x^2),~~~~~~~0 \leq x \leq 1 $$  

Thus, the algorithm given in Exercise 3.9 generates variates from the density $f_e. ~~~ \Box$ 


## Exercise 3.13
### Problem
It can be shown that the mixture in Exercise 3.12 has a Pareto distribution with cdf  
$$F(y)=1-\Big(\frac{\beta}{\beta+y}\Big)^r,~~~~~~~y\geq0$$  

(This is an alternative parameterization of the Pareto cdf given in Exercise
3.3.) Generate 1000 random observations from the mixture with $r=4$ and $\beta=2$. Compare the empirical and theoretical (Pareto) distributions by graphing the density histogram of the sample and superimposing the Pareto density curve.

### Solution
Just do as the problem suggests.  
When $r=4$ and $\beta=2$, the Pareto distribution has a pdf of  
$$f(y)=\frac{64}{(2+y)^5},~~~~~~~y \geq 0 $$


# Homework 3 (2020-10-13)

## Exercise 5.1
### Problem
Compute a Monte Carlo estimate of
$$\int_0^{\pi/3} sin~t~dt$$
and compare your estimate with the exact value of the integral.


### Solution

Use the simple Monte Carlo integration. 
$$X \sim Uniform(0,\pi/3),~~~g(X)=\pi sin(X)/3$$
The exact value is
$$\int_0^{\pi/3} sin~t~dt=cos(0)-cos(\pi/3)=0.5$$

```{r}
set.seed(3333)
monte_carlo_rep <- 100000
uniform_x <- runif(monte_carlo_rep,0,pi/3)
single_estimate <- sin(uniform_x)*pi/3
final_estimate <- mean(single_estimate)
final_estimate
#=0.5006204, very close to the exact value 0.5.
```

## Exercise 5.7
### Problem
Refer to Exercise 5.6. Use a Monte Carlo simulation to estimate $\theta$ by the antithetic variate approach and by the simple Monte Carlo method. Compute an empirical estimate of the percent reduction in variance using the antithetic variate. Compare the result with the theoretical value from Exercise 5.6

### Solution

$$\theta=\int_0^1 e^xdx$$
$$Var(e^U)=E(e^{2U})-(E(e^U))^2=\int_0^1 e^{2x}dx-(\int_0^1 e^{x}dx)^2=\frac{1}{2}(e^2-1)-(e-1)^2=0.2420$$
$$Cov(e^U,e^{1-U})=E(e^{U+1-U)})-E(e^U)E(e^{1-U})=e-\int_0^1e^xdx \int_0^1e^{1-x}dx=-0.2342$$
$$Var((e^U+e^{1-U})/2)=Var(e^U)/4+Var(e^{1-U})/4+Cov(e^U,e^{1-U})/2=0.0039$$

Thus, the theoretical value of percent reduction is $1-0.0039/0.2420=98.39\%$  

Next, we could compute an empirical estimate of the percent reduction in variance by the Monte Carlo simulation.
```{r}
set.seed(2222)
monte_carlo_r <- 100
var_reduc_store <- rep(0,monte_carlo_r)
for (i in 1:monte_carlo_r){
  u <- runif(10000)
  simple_mc <- exp(1)^u
  antithetic <- (exp(1)^u+exp(1)^(1-u))/2
  var_reduc_store[i] <- 1- var(antithetic) / var(simple_mc)
}
mean(var_reduc_store)
# empirical estimate of the percent reduction is 98.38%, very close to the theoretical value 98.39%.
```

## Exercise 5.11
### Problem
If $\hat{\theta}_1$ and $\hat{\theta}_2$ are unbiased estimators of $\theta$, and 
$\hat{\theta}_1$ and $\hat{\theta}_2$ are antithetic, we derived that $c^*=1/2$ is the optimal constant that minimizes the variance of $\hat{\theta}_c=c \hat{\theta}_1+(1-c)\hat{\theta}_2$. Derive $c^*$ for the general case. That is , if $\hat{\theta}_1$ and $\hat{\theta}_2$ are any two unbiased estimators of $\theta$, find the value $c^*$ that minimizes the variance of the estimator $\hat{\theta}_c=c \hat{\theta}_1+(1-c)\hat{\theta}_2$.  


### Solution

\begin{align}
Var(\hat{\theta}_c) &= Var(c \hat{\theta}_1+(1-c)\hat{\theta}_2)\\
&=c^2 Var(\hat{\theta}_1) + (1-c)^2 Var(\hat{\theta}_2) + 2c(1-c) Cov(\hat{\theta}_1,\hat{\theta}_2)\\
&=(Var(\hat{\theta}_1)+Var(\hat{\theta}_2)-2Cov(\hat{\theta}_1,\hat{\theta}_2))c^2+(2Cov(\hat{\theta}_1,\hat{\theta}_2)-2Var(\hat{\theta}_2))c+Var(\hat{\theta}_2)\\
&=(Var(\hat{\theta}_1)+Var(\hat{\theta}_2)-2Cov(\hat{\theta}_1,\hat{\theta}_2))(c+\frac{Cov(\hat{\theta}_1,\hat{\theta}_2)-Var(\hat{\theta}_2)}{Var(\hat{\theta}_1)+Var(\hat{\theta}_2)-2Cov(\hat{\theta}_1,\hat{\theta}_2)})^2+K
\end{align}
,where K is the remaining item irrelevant to c.  

In addition, we know that $0 \leq c \leq 1$,  

Thus, to minimize the variance, $c*= \begin{cases}
0 & ~~~~ \frac{Var(\hat{\theta}_2)-Cov(\hat{\theta}_1,\hat{\theta}_2)}{Var(\hat{\theta}_1)+Var(\hat{\theta}_2)-2Cov(\hat{\theta}_1,\hat{\theta}_2)} <0\\
\frac{Var(\hat{\theta}_2)-Cov(\hat{\theta}_1,\hat{\theta}_2)}{Var(\hat{\theta}_1)+Var(\hat{\theta}_2)-2Cov(\hat{\theta}_1,\hat{\theta}_2)} &~~~~ 0 \leq \frac{Var(\hat{\theta}_2)-Cov(\hat{\theta}_1,\hat{\theta}_2)}{Var(\hat{\theta}_1)+Var(\hat{\theta}_2)-2Cov(\hat{\theta}_1,\hat{\theta}_2)} \leq 1 \\
1 & ~~~~ \frac{Var(\hat{\theta}_2)-Cov(\hat{\theta}_1,\hat{\theta}_2)}{Var(\hat{\theta}_1)+Var(\hat{\theta}_2)-2Cov(\hat{\theta}_1,\hat{\theta}_2)} >1\\
\end{cases}$

$\Box$

# Homework 4 (2020-10-20)

## Exercise 5.13
### Problem
Find two importance functions $f_1$ and $f_2$ that are supported on $(1,\infty)$ and are 'close' to
$$g(x)=\frac{x^2}{\sqrt{2\pi}} e^{-x^2/2},~~~~x>1$$
Which of your two importance functions should produce the smaller variance in estimating
$$\int_1^{\infty} \frac{x^2}{\sqrt{2\pi}}e^{-x^2/2} dx$$
by importance sampling? Explain.

### Solution
Considering the properties of the integrand, I find two functions that are 'close' to the integrand and are supported on $(1,\infty)$. They are $e^{-x/2}$ and $e^{-(x-1)}$. To make them density functions, they need to be multiplied by a constant.

$$f_1(x)=\frac{\sqrt{e}}{2}e^{-x/2},~~~~x>1$$
$$f_2(x)=e^{-(x-1)},~~~~x>1$$


```{r}
set.seed(3333)
g <- function(x) x^2*exp(-x^2/2)/sqrt(2*pi)
f1 <- function(x) sqrt(exp(1))/2*exp(-x/2)
f2 <- function(x) exp(-(x-1))
plot(g,1,5,ylim=c(0,1),ylab='y')
par(new=TRUE)
plot(f1,1,5,xlab='',ylab='',main='',col='blue',ylim=c(0,1))
par(new=TRUE)
plot(f2,1,5,xlab='',ylab='',main='',col='red',ylim=c(0,1))
legend(4,0.9,c("g","f1","f2"), col = c('black','blue','red'),lwd = c(1,1,1))
```

From the figure, it seems that the most nearly constant ratio $g(x)/f(x)$ appears to be $f_2$. Thus, I might prefer $f_2$ for the smallest variance.

```{r}
set.seed(3333)
m <- 100000
se <- rep(0,2)
theta.hat <- rep(0,2)
g <- function(x) x^2*exp(-x^2/2)/sqrt(2*pi)
f1 <- function(x) sqrt(exp(1))/2*exp(-x/2)
f2 <- function(x) exp(-(x-1))

u1 <- runif(m,0,0.5)    # to confirm that all the x1>1
x1 <- -2*log(u1 * 2 / sqrt(exp(1)))    # f1, using inverse transform method
f1g <- g(x1)/f1(x1)
theta.hat[1] <- mean(f1g)
se[1] <- sd(f1g)

x2 <- rexp(m)+1
f2g <- g(x2)/f2(x2)
theta.hat[2] <- mean(f2g)
se[2] <- sd(f2g)

theta.hat
# 0.4014080 0.4002661, both are close to the real value.
se
# 0.302011 0.157805, f2 has smaller standard error, as I expected.
```


## Exercise 5.15
### Problem
Obtain the stratified importance sampling estimate in Example 5.13 and compare it with the result of Example 5.10.

### Solution

```{r}
set.seed(3333)
m <- 10000
g <- function(x) exp(-x - log(1+x^2))
f1 <- function(x) exp(-x)/(1-exp(-1/5))
f2 <- function(x) exp(-x)/(exp(-1/5)-exp(-2/5))
f3 <- function(x) exp(-x)/(exp(-2/5)-exp(-3/5))
f4 <- function(x) exp(-x)/(exp(-3/5)-exp(-4/5))
f5 <- function(x) exp(-x)/(exp(-4/5)-exp(-1))

u <- runif(m)
x1 <- -log(1-u*((1-exp(-1/5))))
x2 <- -log(exp(-1/5)-u*((exp(-1/5)-exp(-2/5))))
x3 <- -log(exp(-2/5)-u*((exp(-2/5)-exp(-3/5))))
x4 <- -log(exp(-3/5)-u*((exp(-3/5)-exp(-4/5))))
x5 <- -log(exp(-4/5)-u*((exp(-4/5)-exp(-1))))

fg1 <- g(x1)/f1(x1)
fg2 <- g(x2)/f2(x2)
fg3 <- g(x3)/f3(x3)
fg4 <- g(x4)/f4(x4)
fg5 <- g(x5)/f5(x5)

fg <- fg1+fg2+fg3+fg4+fg5

mean(fg)
# 0.5248477, very close to the result in Example 5.10

sd(fg)
# 0.01694816, much smaller than the result in Example 5.10

```


## Exercise 6.4
### Problem
Suppose that $X_1,...,X_n$ are a random sample from a from a lognormal distribution with unknown parameters. Construct a 95% confidence interval for the parameter $\mu$. Use a Monte Carlo method to obtain an empirical estimate of the confidence level.

### Solution
Set $n=100, m=100, \mu=5, \sigma=1$
```{r}
library(stats)
set.seed(3333)
n <- 100
m <- 100
l_bound <- rep(0,m)
u_bound <- rep(0,m)
for (i in 1:100) {
  x <- rlnorm(n,5,1)
  mu_hat <- sum(log(x))/n
  q <- qt(1-0.05/2,n-1)
  sigmasqu_hat <- sum((log(x)-mu_hat)^2)/n
  se_hat <- sigmasqu_hat/sqrt(n)
  l_bound[i] <- mu_hat - q*se_hat
  u_bound[i] <- mu_hat + q*se_hat
}
lb <- mean(l_bound)
ub <- mean(u_bound)
c(lb,ub)
# an empirical estimate of the confidence level is [4.808141, 5.199664], while the true value is 5.
```



## Exercise 6.5
### Problem
Suppose a 95% symmetric t-interval is applied to estimate a mean, but the sample data are non-normal. Then the probability that the confidence interval covers the mean is not necessarily equal to 0.95. Use a Monte Carlo experiment to estimate the coverage probability of the t-interval for random samples of $\chi^2(2)$ data with sample size $n = 20$. Compare your t-interval results with the simulation results in Example 6.4. (The t-interval should be more robust to departures from normality than the interval for variance.)

### Solution

Random samples of $\chi^2(2)$ data with sample size $n = 20$ is non-normal, while the random samples in Example 6.4 is normal with mean=0 and sd=2.

```{r}
set.seed(3333)
n <- 20
m <- 10000
lb_non <- rep(0,m)
ub_non <- rep(0,m)
lb_nor <- rep(0,m)
ub_nor <- rep(0,m)
for (i in 1:m) {
  non_normal_x <- rchisq(n,2)
  lb_non[i] <- mean(non_normal_x) - sd(non_normal_x)*qt(1-0.05/2,n-1)/sqrt(n)
  ub_non[i] <- mean(non_normal_x) + sd(non_normal_x)*qt(1-0.05/2,n-1)/sqrt(n)
  normal_x <- rnorm(n,mean=0,sd=2)
  lb_nor[i] <- mean(normal_x) - sd(normal_x)*qt(1-0.05/2,n-1)/sqrt(n)
  ub_nor[i] <- mean(normal_x) + sd(normal_x)*qt(1-0.05/2,n-1)/sqrt(n)
}
cov_prob_non <- mean(lb_non<2 & ub_non>2)
cov_prob_nor <- mean(lb_nor<0 & ub_nor>0)
c(cov_prob_non,cov_prob_nor)
# The normal samples have a coverage probability of 94.9%, which is effective. However, the non-normal samples only have a coverage probability of 91.73%, which suggests that the probability that the confidence interval covers the mean is not necessarily equal to 0.95 for non-normal samples.
```